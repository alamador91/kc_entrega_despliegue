{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLPfSX8NCH62"
      },
      "source": [
        "# Práctica Final: Clasificación de Documentos con Scikit-learn y MLflow\n",
        "\n",
        "En esta práctica, utilizarás un conjunto de datos de Scikit-learn (podeís usar el mismo que en el notebook de Intro MLFlow) para entrenar un modelo de clasificación de documentos. El objetivo es construir un modelo capaz de clasificar automáticamente documentos en categorías predefinidas.\n",
        "\n",
        "Pasos a seguir:\n",
        "\n",
        "    Exploración de Datos: Analiza el conjunto de datos proporcionado para comprender su estructura y contenido.\n",
        "\n",
        "    Preprocesamiento de Texto: Realiza tareas de preprocesamiento de texto, como tokenización y vectorización, para preparar los datos para el modelado.\n",
        "\n",
        "    Entrenamiento del Modelo: Utiliza algoritmos de clasificación de Scikit-learn para entrenar un modelo con los datos preprocesados.\n",
        "\n",
        "    Evaluación del Modelo: Evalúa el rendimiento del modelo utilizando métricas de evaluación estándar como precisión y recall.\n",
        "\n",
        "    Registro de Métricas con MLflow: Utiliza MLflow para registrar métricas y hiperparámetros durante el entrenamiento, facilitando la gestión y comparación de experimentos.\n",
        "\n",
        "\n",
        "Nota: Dado que no voy a poder tener acceso a vuestros logs de MLFlow añadirme las imagenes de la interfaz de MLFlow en el notebook!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import fastapi\n",
        "  import mlflow\n",
        "  import pyngrok\n",
        "except:\n",
        "  !pip install fastapi pyngrok mlflow\n",
        "  import fastapi\n",
        "  import mlflow\n",
        "  import pyngrok\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "Q45VtKu1t4DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment('Classifiers Scikit')\n",
        "#with mlflow.start_run(run_name='Despliegue de Algos'):\n",
        "#  mlflow.log_metric('m1', 1.0)\n",
        "#  mlflow.log_param('n_estimators', 30)\n",
        "\n",
        "get_ipython().system_raw('mlflow ui --port 5000 &')\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "NGROK_AUTH_TOKEN = userdata.get('ngrok_token')\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(addr='5000', proto='http', bind_tls=True)\n",
        "print('El tracking UI:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvSUNO6v1cCJ",
        "outputId": "6c6829f6-168e-444a-de70-745b22f924ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/05/19 18:32:18 INFO mlflow.tracking.fluent: Experiment with name 'Classifiers Scikit' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El tracking UI: https://df33-34-125-202-187.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mlflow_log_score(model_name, metric_name, metric_value):\n",
        "  with mlflow.start_run(run_name=model_name):\n",
        "    mlflow.log_metric(metric_name, metric_value)\n",
        "  #mlflow.log_param('n_stimators', i)\n",
        "  #mlflow.sklearn.log_model(model, 'clf-model')\n",
        ""
      ],
      "metadata": {
        "id": "m6hbJrjs1LAo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valoración de varios modelos de clasificación"
      ],
      "metadata": {
        "id": "0fXoxIlNHX_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4TjQmvM6CH65"
      },
      "outputs": [],
      "source": [
        "# Code source: Gaël Varoquaux\n",
        "#              Andreas Müller\n",
        "# Modified for documentation by Jaques Grobler\n",
        "# License: BSD 3 clause\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.datasets import make_circles, make_classification, make_moons\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "import subprocess\n",
        "import mlflow\n",
        "import time\n",
        "\n",
        "names = [\n",
        "    \"Nearest Neighbors\",\n",
        "    \"Linear SVM\",\n",
        "    \"RBF SVM\",\n",
        "    \"Gaussian Process\",\n",
        "    \"Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"Neural Net\",\n",
        "    \"AdaBoost\",\n",
        "    \"Naive Bayes\",\n",
        "    \"QDA\",\n",
        "]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
        "    SVC(gamma=2, C=1, random_state=42),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
        "    DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    RandomForestClassifier(\n",
        "        max_depth=5, n_estimators=10, max_features=1, random_state=42\n",
        "    ),\n",
        "    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
        "    AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis(),\n",
        "]\n",
        "\n",
        "#init MLFlow\n",
        "#mlflow_ui_process = subprocess.Popen(['mlflow', 'ui', '--port', '5000'])\n",
        "#time.sleep(5)\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
        ")\n",
        "rng = np.random.RandomState(2)\n",
        "X += 2 * rng.uniform(size=X.shape)\n",
        "linearly_separable = (X, y)\n",
        "\n",
        "datasets = [\n",
        "    #make_moons(noise=0.3, random_state=0),\n",
        "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
        "    #linearly_separable,\n",
        "]\n",
        "\n",
        "# iterate over datasets\n",
        "for ds_cnt, ds in enumerate(datasets):\n",
        "    # preprocess dataset, split into training and test part\n",
        "    X, y = ds\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.4, random_state=42\n",
        "    )\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "\n",
        "    # iterate over classifiers\n",
        "    for name, clf in zip(names, classifiers):\n",
        "        clf = make_pipeline(StandardScaler(), clf)\n",
        "        clf.fit(X_train, y_train)\n",
        "        score = clf.score(X_test, y_test)\n",
        "        mlflow_log_score(name, 'Accuracy on test', score)\n",
        "        mlflow_log_score(name, 'acc', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02p_JspNCH68"
      },
      "source": [
        "## Generar .py de funciones y main con al menos dos argumentos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Bv8B6qSVCH69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4697b784-8062-4bdf-d401-09dba54c9bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting calculator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile calculator.py\n",
        "import argparse\n",
        "from cals import *\n",
        "\n",
        "def get_args():\n",
        "  parser = argparse.ArgumentParser(description='Argumentos:')\n",
        "  parser.add_argument('--calc', type=int, help='Operación matemática')\n",
        "  #parser.add_argument('--nom_run', type=str, help='Nombre del run de entrenamiento')\n",
        "  parser.add_argument('--pars', nargs='+', type=int, help='Lista de parámetros(operadores)')\n",
        "  parser.add_argument('--reps', type=int, help='Amppunt of times to repeat using params', default=1)\n",
        "  return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "  args = get_args()\n",
        "  reps = args.reps\n",
        "  params = args.pars\n",
        "  if len(params) < reps*2:\n",
        "    print('Error. Debe dar suficientes parámetros para realizar las operaciones. Los parámetros deben ser al menos el doble que las operaciones')\n",
        "    return\n",
        "  for i in range(reps):\n",
        "    params = args.pars[2*i:2*i+2]\n",
        "    match args.calc:\n",
        "      case 1:\n",
        "        print(suma(*params))\n",
        "      case 2:\n",
        "        print(resta(*params))\n",
        "      case 3:\n",
        "        print(prod(*params))\n",
        "      case 4:\n",
        "        print(divd(*params))\n",
        "      case 5:\n",
        "        print(pow(*params))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9cfnQA-sCH6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c547191-2fff-44a5-b49e-1d8a683453c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cals.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile cals.py\n",
        "def suma(a, b):\n",
        "  return a+b\n",
        "\n",
        "def resta(a, b):\n",
        "  return a-b\n",
        "\n",
        "def prod(a, b):\n",
        "  return a*b\n",
        "\n",
        "def divd(a, b):\n",
        "  return a/b\n",
        "\n",
        "def pow(a, b):\n",
        "  return a**b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "cRQTZZymCH68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed2247d-99fc-4c1f-d6d9-62fc27e041e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error. You must give enough parameters to make operations. 2x params as reps at least\n"
          ]
        }
      ],
      "source": [
        "!python calculator.py \\\n",
        "--calc 1 \\\n",
        "--pars 3 4 2 5 5  \\\n",
        "--reps 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5tqpK-SCH6_"
      },
      "source": [
        "## Práctica parte FastAPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khfxd-jGCH7A"
      },
      "source": [
        "### Para esta parte de la práctica teneis que generar un script con al menos 5 modulos app.get y dos de ellos tienen que ser pipelines de HF.\n",
        "\n",
        "### Parte de la practica se tendra que entregar en capturas de pantalla. Las capturas de pantalla a adjuntas son las siguientes.\n",
        "\n",
        "### 1. Captura de la pantalla docs con al menos 5 modulos.\n",
        "### 2. Captura de cada una de los modulos con la respuesta dentro de docs.\n",
        "### 3. Captura de cada uno de los modulos en la llamada https.\n",
        "### 4. Todo el codigo usado durante el proceso. Notebooks y scripts."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Despliegue"
      ],
      "metadata": {
        "id": "Ox7_9uJft2HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hf.py\n",
        "from fastapi import FastAPI\n",
        "from transformers import pipeline\n",
        "\n",
        "tags_metadata = [\n",
        "    {\n",
        "        \"name\": \"Sentiment Analysis\",\n",
        "        \"description\": \"Este grupo es de analisis de sentimiento\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Q&A\",\n",
        "        \"description\": \"Este grupo es de Question Answering\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Summarization\",\n",
        "        \"description\": \"Este grupo es de resumir\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"items\",\n",
        "        \"description\": \"Manage items. So _fancy_ they have their own docs.\",\n",
        "        \"externalDocs\": {\n",
        "            \"description\": \"Items external docs\",\n",
        "            \"url\": \"https://fastapi.tiangolo.com/\",\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "app = FastAPI(openapi_tags=tags_metadata)\n",
        "app._ctx = ''\n",
        "\n",
        "@app.get('/sa', tags=['Sentiment Analysis'])\n",
        "def sa(prompt):\n",
        "  sa = pipeline('sentiment-analysis')\n",
        "  return sa(prompt)\n",
        "\n",
        "\"\"\"\n",
        "Fija un contexto para el Q&A\n",
        "\"\"\"\n",
        "@app.get('/set_ctx', tags=['Q&A'])\n",
        "def set_context(ctx):\n",
        "  app._ctx = ctx\n",
        "  return f'El contexto ha sido fijado: {app._ctx}'\n",
        "\n",
        "@app.get('/get_ctx', tags=['Q&A'])\n",
        "def get_ctx():\n",
        "  return app._ctx\n",
        "\n",
        "@app.get('/qa', tags=['Q&A'])\n",
        "def qa(prompt):\n",
        "  if app._ctx == '':\n",
        "    return 'Debe fijar un contexto primero usando set_ctx'\n",
        "  qa = pipeline('question-answering', model='BSC-LT/roberta-base-bne')\n",
        "  return qa(prompt, app._ctx)\n",
        "\n",
        "app._max_len = 512\n",
        "@app.get('/set_max_length', tags=['Summarization'])\n",
        "def set_max_len(length):\n",
        "  app._max_len = lenght\n",
        "  return f'Max length set to: {_max_length}'\n",
        "\n",
        "\n",
        "@app.get('/sum', tags=['Summarization'])\n",
        "def sum(text):\n",
        "  import torch\n",
        "  from transformers import BertTokenizerFast, EncoderDecoderModel\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  ckpt = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(ckpt)\n",
        "  model = EncoderDecoderModel.from_pretrained(ckpt).to(device)\n",
        "\n",
        "  inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=app._max_len, return_tensors=\"pt\")\n",
        "  input_ids = inputs.input_ids.to(device)\n",
        "  attention_mask = inputs.attention_mask.to(device)\n",
        "  output = model.generate(input_ids, attention_mask=attention_mask)\n",
        "  return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  generate_summary(text)\n",
        "\n",
        "#@app.get('/trad/{en}')\n",
        "#def trad(en):\n",
        "#  from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "#\n",
        "#  tr = pipeline('translation_EN_to_ES', model='jbochi/madlad400-3b-mt')\n",
        "#  return tr(en)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ryinuvp16he6",
        "outputId": "0a47f517-555b-4ad9-9c97-8b32aabe5a8b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok, conf\n",
        "NGROK_TOKEN = userdata.get('ngrok_token')\n",
        "conf.get_default().auth_token = NGROK_TOKEN #cada uno os lo debéis generar en ngrok.\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url+'/docs')\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fdyGmVy6xyC",
        "outputId": "6686124f-2de4-47ae-f2f2-b6f44d805e62"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://1b5c-34-125-202-187.ngrok-free.app/docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn hf:app --port 8000"
      ],
      "metadata": {
        "id": "CyLtpo4JuXKR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}